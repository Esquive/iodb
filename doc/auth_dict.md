Auth Dictionary and IODB
=============================

Using Merkle Tree, Skip List or similar data structure for authentication  IODB brings following problems:

- Data structure has to be regenerated for each update. It is not enough to prove that key existed on older version, but we also need to prove that key was not deleted in recent update.

- Binary trees and Skip Lists are hard to implement on top of filesystem, because they generate lot of random IO. Random 
reads are solved by SSDs, but random writes are not. All papers we have describe in-memory implementations.
 
- Non of those data structures supports versioning and rollbacks. Adding those would complicate design.

This 'paper' describes authenticated dictionary integrated into LSM Tree used in IODB. 
It uses append-only file operations and should have great performance on filesystems. 
It discusses two authentication methods (signature and root hash), IODB can work with both of them.

IODB architecture overview
---------------------------

### Log
IODB stores all updates in append-only log. Each update has link to previous update. To find a key,  database traverses updates (from most recent), until key is found or end (when empty store was created) is reached. 

To confirm key does not exist user has to traverse entire log.  
Deleted keys are handled by tombstones (special type of value).  
To confirm non-existence of recently deleted key, user only has to traverse log until tombstone is found.  
Finding double-spend keys should not be a performance problem.

### Merge Tables
Over time number of updates grows and traversing entire log will become too slow.
For that IODB has *Merge Tables*; sorted table of keys, populated  by replaying all updates from log. 

Merge tables are generated by background compaction. When `get()` operation traverses log to find key, and it finds Merge Table at given version, it will lookup key in Merge Table and stop log traversal.  

*Merge Table* has upper and lower bounds. Initially (with empty log) it starts with negative and positive infinity bounds. When merge table grows too big (over 1MB) it is split into two new Merge Tables. New tables cover the same interval as original table. 
 
If keys are deleted and single Merge Table becomes too small, it merges with neighbours and its bounds are expanded to cover more keys. 


Signed versus Hashed authentication
--------------------------------------------

Merkle Trees use hash value for authentication. We know Root Hash of an tree, and that is enough to verify all values. In this case authentication proof is a sequence of nodes (or operations) which matches the Root Hash. 
To perform independent verification of Root Hash we need all data within from Merkle Tree.

There is a second way to authenticate tree content. It relies on public-private keys. Updates are signed by private key. Proof is to provide sequence of nodes (or operations) with valid signature. 
 
Signature based authentication is suggested in [1] and some others. Quote:
 
*Many of these designs can be implemented using the dynamic authenticated dictionary paradigm (DAD) [26, 19]. A DAD is a key/value dictionary that permits updates. Updates are signed by trusted authors. The dictionary is stored on untrusted servers and queried by clients. Query responses are authenticated by the authorâ€™s digital signature.*

This reduces communications overhead. Proof should be smaller compared to hash based authentication. It also simplifies design, since Root Hash and its changes does not have to be distributed to client, it uses static public key. 

IODB and signature based auth
-----------------------------------------------------------

Authentication based on signature uses author with private and public key. Author signs updates with his/her private key,
proof is done by comparing data signature with public key.  

In this scenario author signs all updates made on database. Each update generates new entry in append-only log  (Update Entry). Following data are signed by private key on each update:

- Version ID of current update
- Version ID of previous update
- Signature (or hash) of previous update 
- Deleted keys
- Updated key-value pairs

Every Update entry contains signature of previous Update Entry, and indirectly of all older Update Entries. 
It is not possible to modify any older Update Entry, without invalidating chain of signatures. 

### Proof by Log Replay

Prof of existence is done following way:
 
- client asks for key at version V
- IODB traverses Log Entries, until key (or tombstone) is found at version W. 
- IODB sends answer (exist or not) together with all Update Entries between version V and W
- Clients verifies signature of received data 
- Client searches Update Entries the same way as IODB 
 
### Proof with Merged Tables

Log replay is not practical as log size grows. Merged Tables can be included in proof. 

Merged Tables are generated by compaction in background thread. Merge Table is generated by replaying 
update log, result is set of all existing keys  at version V. To generate Merged Table we need all Update Entries,
since store creation or since older Merged Table. 

Second option to generate Merged Table is to use an older Merge Table at version W with the same bounds.
Using older Merged Table will eliminate older Update Entries, it already contains all existing keys.
Than we replay all updates from version W to version V. 

Merged Table is signed by author. Compaction runs on background, so database needs some way to contact author and ask him for signature.
Database will send author list of Update Entries and (optionally) existing Merged Table. 
Author will merge all data the same way as compaction, and signs result with private key.
All data send to author are already signed with his/her private key. So author does not store any data (or state) 
for verification.  Author just takes already signed data, performs deterministic merge and signs the result. 

Prof of existence is done by following way:
 
- client asks for key at version V
- IODB traverses Log Entries, until key (or tombstone) is found at version W. 
- IODB sends answer (exist or not) together with all Update Entries between version V and W
- If key was found in signed Merged Table, IODB will send this table to client as well
- Client verifies that signature of received data are matching authors public key 
- Client searches Update Entries and Merged Table the same way as IODB  
 

IODB and hash based auth
----------------------------------

Hash based authentication is similar to signature based auth. Update Entries are stored in log. Each entry contains
hash of previous Update Entry. It is not possible to modify older updates, without invalidating hash of newer updates. 

Client can verify hash of given Version ID by replaying all updates since store was created. 

Proof of existence is done following way:  
- client asks for key at version V
- IODB traverses Log Entries, until key (or tombstone) is found at version W. 
- IODB sends answer (exist or not) together with all Update Entries between version V and W
- Client verifies chain of hash codes is matching his own hash code
- Client searches Update Entries the same way as IODB and verifies key exists.  
 
### Merged Tables and hash based auth

Merged Table content is result of replaying all Update entries since store was created. If client has entire Update
Log, it can reconstruct and verify any Merged Table send to him/her. 
 
Also if client only has root hash code for given version, database can prove any Merged Table by sending entire Update Log
which matches root hash code. Client can than replay entire Update Log and reconstruct Merged Table.

The problem is how client can verify Merged Table without full Update Log. With signature based auth 
each table is signed by author. In hash based auth we make MT hash code part of blockchain.
Compaction process generates Merge Table, after that it will wait for next update and 
add MT hash code into update. Merge Table hash code will be
included in blockchain.

Verification is than done following way:

- client asks for key at version V
- IODB traverses Log Entries, until key (or tombstone) is found at version W. 
- IODB sends answer (exist or not) together with all Update Entries between version V and W
- If key was found in signed Merged Table, IODB will send this table to client as well
- Client verifies that hash codes of received data are matching Root Hash Code 
- Client searches Update Entries and Merged Table the same way as IODB
 
IODB and size of proof
-----------------------------

Size of proof is similar for Signature and Hash based authentication

Lets assume that:
- key size is 32 bytes 
- every update modifies 1000 keys 
- Average Merged Table (MT) is 1MB big, and is regenerated every 100 updates
- values are replaced by their Hash codes for authentication

In best case scenario (MT was just regenerated) size of proof is equal to size of MT (1MB). 
In worst case scenario ()MT is just about to be regenerated). The size of proof for single key is:
```
32 bytes * 1000 keys * 100 updates + 1MB size of MT = 4.2 MB
```

Size of MT and regeneration frequency can be tuned to reduce proof size. But I think values above correspond to realistic load. 

Data size may seem big, but we should keep two things on mind:

- It does not require any CPU or random IO to serve proof, it is just a list of files. Proof  can be even served (and mirrored) by static webservers.
 
- Batch authentication of group of keys does not require much more data. Update Log and Merged Tables are shared between keys.   



Rollback attack
-----------------------

If version IDs are reused after rollback, attacker could pretend that key from older update (rolled back) is in current update. This is the case for signature based verification, but should not be a problem for hash based verification. 

Anyway I recommend we do not reuse version IDs. New update should always get an unique version ID. For example if there are versions 1,2,3,4,5 and we rollback from 5 to 3, next update should continue with version ID 6. 



Papers
---------------

[Authenticated dictionaries: Real-world costs and tradeoffs](https://www.semanticscholar.org/paper/Authenticated-Dictionaries-Real-world-Costs-and-Crosby-Wallach/8ffd4ff1891728a9fe9bf75cfe9924c4a3c800e9/pdf)

Scott A Crosby and Dan S. Wallach, Rice University, December 14, 2010

[Implementation of an Authenticated Dictionary with Skip Lists and Commutative Hashing](https://www.cs.jhu.edu/~goodrich/cgc/pubs/discex2001.pdf)

Michael T. Goodrich, Roberto Tamassia, Andrew Schwerin


[Efficient Sparse Merkle Trees Caching Strategies and Secure (Non-)Membership Proofs](https://eprint.iacr.org/2016/683)

Rasmus Dahlberg 1 , Tobias Pulls 1 , and Roel Peeters 2


